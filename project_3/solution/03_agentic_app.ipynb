{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d80ddd8",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "68aa44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from utils import chat_interface\n",
    "import os\n",
    "import math\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import MessagesState\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from pydantic import BaseModel, Field, ConfigDict\n",
    "from typing import Dict, Any, List, Annotated, Literal, Tuple, Optional\n",
    "from langchain_core.messages import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074877bd",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "837f6e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_retrieval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6da4d",
   "metadata": {},
   "source": [
    "### Setup LLMs for different Technical Support Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd95aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# llm_base_url = \"https://openai.vocareum.com/v1\"\n",
    "llm_base_url = \"https://api.openai.com/v1\"\n",
    "\n",
    "\n",
    "llm_small = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    "    base_url=llm_base_url,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "llm_medium = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.0,\n",
    "    base_url=llm_base_url,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "llm_large = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.0,\n",
    "    base_url=llm_base_url,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "embeddings_fn = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    "    base_url=llm_base_url,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac44299",
   "metadata": {},
   "source": [
    "### Create Vector Database Connection for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "302b9724",
   "metadata": {},
   "outputs": [],
   "source": [
    "chromadb_directory = \"vectorstore\"\n",
    "collection_name = \"knowledge_vecotr_store\"\n",
    "vector_store = Chroma(\n",
    "    collection_name=collection_name,\n",
    "    embedding_function=embeddings_fn,\n",
    "    persist_directory=chromadb_directory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa125de1",
   "metadata": {},
   "source": [
    "## Agent State Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb6e5de",
   "metadata": {},
   "source": [
    "State Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketState(MessagesState):\n",
    "    \"\"\"State for the IT ticket workflow.\"\"\"\n",
    "    query: HumanMessage\n",
    "    current_answers: AIMessage\n",
    "    # retrieved documents\n",
    "    documents: List[Document]\n",
    "    # routing signals\n",
    "    priority: Literal['normal', 'urgent', 'vip'] = 'normal'\n",
    "    intent: Literal['how_to', 'troubleshooting_basic', 'status_check', \n",
    "                    'security', 'billing_payment', 'outage', 'others'] = 'others'\n",
    "    retrieval_score: float = 0.0\n",
    "    answer_confidence: float = 0.0\n",
    "    sentiment: float = 0.0\n",
    "    # control/status\n",
    "    assigned_group: Literal['L1', 'L2', 'L3', 'human'] = 'L1'\n",
    "    eval_status: Literal['resolved', 'needs_clarification', 'escalate'] = 'needs_clarification'\n",
    "    turns_at_level: int = 0\n",
    "    escalation_reason: Optional[Literal['low_confidence',\n",
    "                                        'high_impact_or_sensitive',\n",
    "                                        'unresolved_or_negative',\n",
    "                                        'sla_breach']] = None\n",
    "    # SLA/aging\n",
    "    level_entered_at: datetime = datetime.now(timezone.utc)\n",
    "    sla_minutes: int = 3  # example per level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e4584",
   "metadata": {},
   "source": [
    "Standardized Output for Subagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7e2d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentOutput(BaseModel):\n",
    "    \"\"\"Standardized output to ensure it is from the possible values\"\"\"\n",
    "    intent: Annotated[Literal['how_to','troubleshooting_basic','status_check','security','refund_high','outage','others'], \n",
    "                      Field(description=\"The intent of the IT ticket\")]\n",
    "    \n",
    "class SentimentOutput(BaseModel):\n",
    "    \"\"\"Standardized output to ensure it is from the possible values\"\"\"\n",
    "    sentiment: Annotated[float, Field(description=\"The user's sentiment\",\n",
    "                                      ge=-1.0,\n",
    "                                      le=1.0)]\n",
    "\n",
    "class ConfidenceOutput(BaseModel):\n",
    "    \"\"\"Standardized output to ensure it is from the possible values\"\"\"\n",
    "    confidencet: Annotated[float, Field(description=\"The confidence of answer toward the query\",\n",
    "                                      ge=0.0,\n",
    "                                      le=1.0)]\n",
    "\n",
    "# ChromaDB Similarity Search Output Structure        \n",
    "class VectorDocument(BaseModel):\n",
    "    \"\"\"The contents of a retrieved LangChain Document.\"\"\"\n",
    "    id: int\n",
    "    page_content: str\n",
    "    metadata: Dict[str, Any]\n",
    "\n",
    "class SearchResultItem(BaseModel):\n",
    "    \"\"\"Represents the tuple (Document, score) as a JSON object.\"\"\"\n",
    "    document: VectorDocument = Field(description=\"The document content and metadata.\")\n",
    "    score: float = Field(description=\"The L2 distance.\")\n",
    "\n",
    "class QueryAttributeOutput(BaseModel):\n",
    "    \"\"\"Standardized output\"\"\"\n",
    "    intent: Annotated[Literal['how_to','troubleshooting_basic','status_check','security','refund_high','outage','others'], \n",
    "                      Field(description=\"The intent of the IT ticket\")]\n",
    "    sentiment: Annotated[float, Field(description=\"The user's sentiment\",\n",
    "                                      ge=-1.0,\n",
    "                                      le=1.0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18650b",
   "metadata": {},
   "source": [
    "# Agent Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6e889a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def knowledge_retrival(query: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Evaluate whether the knowledge base has enough content to address the query and return the relevant content indices.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query string\n",
    "    \n",
    "    Returns:\n",
    "        retrival (List[tuple[Document,float]]): Response from the ChromaDB database for the retrieved documents and the corresponding\n",
    "                                                distance measure.\n",
    "    \"\"\"\n",
    "    results = vector_store.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=num_retrieval,\n",
    "    )\n",
    "\n",
    "    # Convert the output to recognizable types for Pydantic to work\n",
    "    retrival = []\n",
    "    for doc, score in results:\n",
    "        retrival.append({\n",
    "            \"document\": {\n",
    "                \"id\": int(doc.id),\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"tags\": doc.metadata.get(\"tags\",\"\")\n",
    "            },\n",
    "            \"score\": score\n",
    "        })\n",
    "    return retrival\n",
    "\n",
    "def run_retrieval(query: str):\n",
    "    return {\"query\": query, \"retrival\": knowledge_retrival.func(query)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f08124",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def intent_check(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Evaluate the intent of the query for routing decision.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query string\n",
    "    \n",
    "    Returns:\n",
    "        intent (str): One of ['how_to', 'troubleshooting_basic', 'status_check',\n",
    "                              'security', 'billing_payment', 'outage', 'others']\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt_template = \"\"\"\n",
    "        You are an expert evaluating the intent of IT ticket submitted by users and you are suppose to\n",
    "        classify it as one of the following intent: ['how_to', 'troubleshooting_basic', 'status_check',\n",
    "        'security', 'billing_payment', 'outage', 'others'].\n",
    "\n",
    "        Output a single one of the above intent in the list, do not provide any explanation or description\n",
    "        to your answer.\n",
    "\n",
    "        Here is the user query:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template = prompt_template,\n",
    "        input_variables = [\"query\"],\n",
    "        ).invoke({\"query\": query})\n",
    "    response = llm_medium.with_structured_output(IntentOutput).invoke(prompt)\n",
    "    intent = response.intent\n",
    "    return intent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d3551a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def sentiment_analyzer(query: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the sentiment of the input query and output a score between -1 to 1.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query string\n",
    "    \n",
    "    Returns:\n",
    "        sentiment (float): Score between -1 to 1, where -1 is upset/angry/negative where 1 is happy/optimisitic/positive\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "        You are an expert in marketing and assessing the sentiment of the customer.  You need to provide a score between\n",
    "        -1 to 1 where -1 is upset/angry/negative where 1 is happy/optimisitic/positive.\n",
    "\n",
    "        Output a single number, do not provide any explanation or description to your answer.\n",
    "\n",
    "        Here is the user query:\n",
    "        {query}\n",
    "        \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template = prompt_template,\n",
    "        input_variables = [\"query\"],\n",
    "        ).invoke({\"query\": query})\n",
    "    response = llm_medium.with_structured_output(SentimentOutput).invoke(prompt)\n",
    "    sentiment = response.sentiment\n",
    "    return sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "302413ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def confidence_evaluator(query: str, answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Evaluate the confidence of the answer towards answering the query by providing a score between 0 to 1.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query string\n",
    "        answer (str): The answer provided by the IT support\n",
    "    \n",
    "    Returns:\n",
    "        confidence (float): Score between 0 to 1, where 0 corresponse to an answer where it is completely irrelevant to\n",
    "        the querey, where 1 is spot on and it solves the user's query.\n",
    "    \"\"\"\n",
    "    prompt_template = \"\"\"\n",
    "        You are an expert in quality control to evaluate whether the support team had been doing a good job for providing\n",
    "        the relevant response to the user.  You need to provide a score between 0 to 1, where 0 corresponse to an answer \n",
    "        where it is completely irrelevant to the querey, where 1 is spot on and it solves the user's query.\n",
    "\n",
    "        Here is the user query:\n",
    "        {query}\n",
    "\n",
    "        Here is the support team's response:\n",
    "        {answer}\n",
    "        \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template = prompt_template,\n",
    "        input_variables = [\"query\",\"answer\"],\n",
    "        ).invoke({\"query\": query, \"answer\": answer})\n",
    "    response = llm_medium.with_structured_output(ConfidenceOutput).invoke(prompt)\n",
    "    confidence = response.confidence\n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "6d8674d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_initial_evaluator = llm_medium.bind_tools([knowledge_retrival,intent_check,sentiment_analyzer])\n",
    "llm_answer_evaluator = llm_medium.bind_tools([confidence_evaluator])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4281a8d",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a21ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_assessment_agent(state: TicketState):\n",
    "    query = state[\"query\"]\n",
    "    # Obtain Retrival Score and Documents\n",
    "    retrivals = vector_store.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=num_retrieval,\n",
    "        )\n",
    "\n",
    "    documents = [ retrival[0] for retrival in retrivals ]\n",
    "    similarity = [ 1-(retrival[1]/math.sqrt(2)) for retrival in retrivals ]  # ChromaDB uses L2 distance, it requires to perform this operation to convert to similarity\n",
    "    max_similarity_score = max(similarity)\n",
    "\n",
    "    # Intent Checking and Sentiment Analysis\n",
    "    llm_initial_evaluator = llm_medium.bind_tools([intent_check,sentiment_analyzer])\n",
    "    prompt_template = \"\"\"\n",
    "        You are a IT ticket evaluation agent, given the user query:\n",
    "        {query}\n",
    "        Use the following tools to provide insights for the user query:\n",
    "            (1) Use `intent_check` to identify the \"intent\" attribute of the given query\n",
    "            (2) Use `sentiment_analyzer` to estimate the \"sentiment\" of the given query\n",
    "        \"\"\"\n",
    "    prompt = PromptTemplate(\n",
    "        template = prompt_template,\n",
    "        input_variables = [\"query\"],\n",
    "        ).invoke({\"query\": query})\n",
    "    response = llm_initial_evaluator.with_structured_output(QueryAttributeOutput).invoke(prompt)\n",
    "    \n",
    "    ai_message = AIMessage(content=f\"intent: {response.intent_check}, retrival_score: {max_similarity_score}, sentiment: {response.sentiment}\")\n",
    "\n",
    "    return {\"intent\": response.intent_check,\n",
    "            \"retrival_score\": max_similarity_score,\n",
    "            \"sentiment\": response.sentiment,\n",
    "            \"documents\": documents,\n",
    "            \"messages\": ai_message}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f8efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_TOPICS = {\"how_to\",\"troubleshooting_basic\",\"status_check\"}\n",
    "L2_TOPICS = {\"others\",\"billing_payment\"}\n",
    "L3_TOPICS = {\"outage\",\"security\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad2d366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_agent(state: TicketState):\n",
    "    score = state[\"retrieval_score\"]\n",
    "    intent = state[\"intent\"]\n",
    "    if score < 0.4 or intent in L3_TOPICS:\n",
    "        return \"L3\"\n",
    "    if intent in L2_TOPICS:\n",
    "        return \"L2\"\n",
    "    return \"L1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96608ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escalation_agent(state: TicketState):\n",
    "    # Evaluate answer confidence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_agent(state: TicketState):\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are an assistant for question-answering tasks.\"),\n",
    "        (\"human\", \"\"\"\n",
    "            Use the following pieces of retrieved context to answer the question.\n",
    "            If you don't know the answer, just say that you don't know.\n",
    "            Use three sentences maximum and keep the answer concise.\n",
    "            # Question: -> {question}\n",
    "            # Context: -> {context}\n",
    "            # Answer:\n",
    "            \"\"\"\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    messages = template.invoke(\n",
    "        {\"context\": docs_content, \"question\": question}\n",
    "        ).to_messages()\n",
    "\n",
    "    ai_message = llm_small.invoke(messages)\n",
    "\n",
    "    return {\"messages\": ai_message,\n",
    "            \"current_answer\": ai_message}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922e9790",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d09a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDEALLY YOUR ONLY IMPORT HERE IS:\n",
    "# from agentic.workflow import orchestrator\n",
    "\n",
    "from agentic.workflow import orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1257bf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface(orchestrator, \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe200f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(orchestrator.get_state_history(\n",
    "    config = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": \"1\",\n",
    "        }\n",
    "    }\n",
    "))[0].values[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41e9f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9be7ad6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f3de5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39006a38",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e609113",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "1a62993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def knowledge_retrival(query: str) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Evaluate whether the knowledge base has enough content to address the query and return the relevant content indices.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query string\n",
    "    \n",
    "    Returns:\n",
    "        retrival (List[tuple[Document,float]]): Response from the ChromaDB database for the retrieved documents and the corresponding\n",
    "                                                distance measure.\n",
    "    \"\"\"\n",
    "    results = vector_store.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=num_retrieval,\n",
    "    )\n",
    "\n",
    "    # Convert the output to recognizable types for Pydantic to work\n",
    "    retrival = []\n",
    "    for doc, score in results:\n",
    "        retrival.append({\n",
    "            \"document\": {\n",
    "                \"id\": int(doc.id),\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"tags\": doc.metadata.get(\"tags\",\"\")\n",
    "            },\n",
    "            \"score\": score\n",
    "        })\n",
    "    return retrival\n",
    "\n",
    "def run_retrieval(query: str):\n",
    "    return {\"query\": query, \"retrival\": knowledge_retrival.func(query)}\n",
    "\n",
    "class VectorDocument(BaseModel):\n",
    "    \"\"\"The contents of a retrieved LangChain Document.\"\"\"\n",
    "    id: int\n",
    "    page_content: str\n",
    "    tags: str\n",
    "\n",
    "class SearchResultItem(BaseModel):\n",
    "    \"\"\"Represents the tuple (Document, score) as a JSON object.\"\"\"\n",
    "    document: VectorDocument = Field(description=\"The document content and metadata.\")\n",
    "    score: float = Field(description=\"The L2 distance.\")\n",
    "\n",
    "class QueryAttributeOutput(BaseModel):\n",
    "    retrival: List[SearchResultItem] = Field(min_length=num_retrieval, description=\"List of retrieved document and score pairs.\")\n",
    "\n",
    "query = \"Hi, I have a question regarding the billing for the events I attended last month. There seems to be a discrepancy in the charges, and I would like to understand the breakdown of the costs. Could you please provide a detailed billing statement? Thank you.\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Return the provided retrievals verbatim in the output schema.\\nRetrievals:\\n{retrival}\"\n",
    "    )\n",
    "\n",
    "pipeline = (\n",
    "    RunnableLambda(lambda q: {\"retrival\": knowledge_retrival.func(q)})\n",
    "    | prompt\n",
    "    | llm_medium.with_structured_output(QueryAttributeOutput)\n",
    "    )\n",
    "retrivals = pipeline.invoke(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "04992f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResultItem(document=VectorDocument(id=19, page_content='How to Handle Billing Discrepancies: For billing issues:\\n\\n- Review your billing history in the CultPass app\\n- Contact support if discrepancies are found\\n- Provide transaction details for faster resolution\\n\\n**Suggested phrasing:**\\n\"If you notice any billing discrepancies, review your history and contact support with transaction details.\\'', tags='billing, escalation, support'), score=1.0416605472564697),\n",
       " SearchResultItem(document=VectorDocument(id=5, page_content='Understanding CultPass Pricing: CultPass offers flexible pricing options:\\n\\n- Monthly subscription: Access to 4 experiences per month\\n- Annual subscription: Discounted rate for a year-long access\\n- Additional fees may apply for premium events\\n\\n**Suggested phrasing:**\\n\"Explore our flexible pricing plans, including monthly and annual subscriptions. Note that some premium events may incur additional fees.\\'', tags='pricing, subscription, benefits'), score=1.2668540477752686),\n",
       " SearchResultItem(document=VectorDocument(id=14, page_content='Handling Event Overbooking: In case of overbooking:\\n\\n- Notify affected users via email\\n- Offer alternative dates or events\\n- Provide a refund if no alternatives are suitable\\n\\n**Suggested phrasing:**\\n\"We apologize for any inconvenience due to overbooking. Please check your email for alternative options or refund details.\\'', tags='events, booking, cancelation'), score=1.3216909170150757),\n",
       " SearchResultItem(document=VectorDocument(id=11, page_content='How to Access Premium Events: Accessing premium events requires:\\n\\n- A valid CultPass subscription\\n- Payment of any additional fees\\n- Booking through the CultPass app\\n\\n**Suggested phrasing:**\\n\"Access premium events by ensuring your subscription is active and any additional fees are paid. Book through the app.\\'', tags='access, events, booking'), score=1.3562488555908203),\n",
       " SearchResultItem(document=VectorDocument(id=1, page_content='What\\'s Included in a CultPass Subscription: Each user is entitled to 4 cultural experiences per month, which may include:\\n- Art exhibitions\\n- Museum entries\\n- Music concerts\\n- Film screenings and more\\n\\nSome premium experiences may require an additional fee (visible in the app).\\n\\n**Suggested phrasing:**\\n\"Your CultPass subscription includes 4 curated experiences each month. You can choose from museums, concerts, film events and more. Premium events may have an extra cost, which is shown during reservation.\\'', tags='subscription, benefits, pricing, access'), score=1.4075515270233154)]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrivals.retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7537da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"Hi, I have a question regarding the billing for the events I attended last month. There seems to be a discrepancy in the charges, and I would like to understand the breakdown of the costs. Could you please provide a detailed billing statement? Thank you.\"\n",
    "\n",
    "retrivals = vector_store.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=num_retrieval,\n",
    "    )\n",
    "similarity = [ 1-(retrival[1]/math.sqrt(2)) for retrival in retrivals ]  # ChromaDB uses L2 distance, it requires to perform this operation to convert to similarity\n",
    "max_similarity_score = max(similarity)\n",
    "\n",
    "# Intent Checking and Sentiment Analysis\n",
    "llm_initial_evaluator = llm_medium.bind_tools([intent_check,sentiment_analyzer])\n",
    "prompt_template = \"\"\"\n",
    "    You are a IT ticket evaluation agent, given the user query:\n",
    "    {query}\n",
    "\n",
    "    Use the following tools to provide insights for the user query:\n",
    "        (1) Use `intent_check` to identify the \"intent\" attribute of the given query\n",
    "        (2) Use `sentiment_analyzer` to estimate the \"sentiment\" of the given query\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables = [\"query\"],\n",
    "    ).invoke({\"query\": query})\n",
    "response = llm_initial_evaluator.with_structured_output(QueryAttributeOutput).invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "dce203bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='19', metadata={'tags': 'billing, escalation, support'}, page_content='How to Handle Billing Discrepancies: For billing issues:\\n\\n- Review your billing history in the CultPass app\\n- Contact support if discrepancies are found\\n- Provide transaction details for faster resolution\\n\\n**Suggested phrasing:**\\n\"If you notice any billing discrepancies, review your history and contact support with transaction details.\"'),\n",
       "  1.0416462421417236),\n",
       " (Document(id='5', metadata={'tags': 'pricing, subscription, benefits'}, page_content='Understanding CultPass Pricing: CultPass offers flexible pricing options:\\n\\n- Monthly subscription: Access to 4 experiences per month\\n- Annual subscription: Discounted rate for a year-long access\\n- Additional fees may apply for premium events\\n\\n**Suggested phrasing:**\\n\"Explore our flexible pricing plans, including monthly and annual subscriptions. Note that some premium events may incur additional fees.\"'),\n",
       "  1.2668156623840332),\n",
       " (Document(id='14', metadata={'tags': 'events, booking, cancelation'}, page_content='Handling Event Overbooking: In case of overbooking:\\n\\n- Notify affected users via email\\n- Offer alternative dates or events\\n- Provide a refund if no alternatives are suitable\\n\\n**Suggested phrasing:**\\n\"We apologize for any inconvenience due to overbooking. Please check your email for alternative options or refund details.\"'),\n",
       "  1.3216485977172852),\n",
       " (Document(id='11', metadata={'tags': 'access, events, booking'}, page_content='How to Access Premium Events: Accessing premium events requires:\\n\\n- A valid CultPass subscription\\n- Payment of any additional fees\\n- Booking through the CultPass app\\n\\n**Suggested phrasing:**\\n\"Access premium events by ensuring your subscription is active and any additional fees are paid. Book through the app.\"'),\n",
       "  1.3562390804290771),\n",
       " (Document(id='1', metadata={'tags': 'subscription, benefits, pricing, access'}, page_content='What\\'s Included in a CultPass Subscription: Each user is entitled to 4 cultural experiences per month, which may include:\\n- Art exhibitions\\n- Museum entries\\n- Music concerts\\n- Film screenings and more\\n\\nSome premium experiences may require an additional fee (visible in the app).\\n\\n**Suggested phrasing:**\\n\"Your CultPass subscription includes 4 curated experiences each month. You can choose from museums, concerts, film events and more. Premium events may have an extra cost, which is shown during reservation.\"'),\n",
       "  1.4075047969818115)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0766e3eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryAttributeOutput(retrival=[SearchResultItem(document=VectorDocument(id=19, page_content='How to Handle Billing Discrepancies: For billing issues:\\n\\n- Review your billing history in the CultPass app\\n- Contact support if discrepancies are found\\n- Provide transaction details for faster resolution\\n\\n**Suggested phrasing:**\\n\"If you notice any billing discrepancies, review your history and contact support with transaction details.\"', tags='billing, escalation, support'), score=1.0415587425231934), SearchResultItem(document=VectorDocument(id=5, page_content='Understanding CultPass Pricing: CultPass offers flexible pricing options:\\n\\n- Monthly subscription: Access to 4 experiences per month\\n- Annual subscription: Discounted rate for a year-long access\\n- Additional fees may apply for premium events\\n\\n**Suggested phrasing:**\\n\"Explore our flexible pricing plans, including monthly and annual subscriptions. Note that some premium events may incur additional fees.\"', tags='pricing, subscription, benefits'), score=1.266741394996643), SearchResultItem(document=VectorDocument(id=14, page_content='Handling Event Overbooking: In case of overbooking:\\n\\n- Notify affected users via email\\n- Offer alternative dates or events\\n- Provide a refund if no alternatives are suitable\\n\\n**Suggested phrasing:**\\n\"We apologize for any inconvenience due to overbooking. Please check your email for alternative options or refund details.\"', tags='events, booking, cancelation'), score=1.3215640783309937), SearchResultItem(document=VectorDocument(id=11, page_content='How to Access Premium Events: Accessing premium events requires:\\n\\n- A valid CultPass subscription\\n- Payment of any additional fees\\n- Booking through the CultPass app\\n\\n**Suggested phrasing:**\\n\"Access premium events by ensuring your subscription is active and any additional fees are paid. Book through the app.\"', tags='access, events, booking'), score=1.3561773300170898), SearchResultItem(document=VectorDocument(id=1, page_content='What\\'s Included in a CultPass Subscription: Each user is entitled to 4 cultural experiences per month, which may include:\\n- Art exhibitions\\n- Museum entries\\n- Music concerts\\n- Film screenings and more\\n\\nSome premium experiences may require an additional fee (visible in the app).\\n\\n**Suggested phrasing:**\\n\"Your CultPass subscription includes 4 curated experiences each month. You can choose from museums, concerts, film events and more. Premium events may have an extra cost, which is shown during reservation.\"', tags='subscription, benefits, pricing, access'), score=1.4074517488479614)])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi, I have a question regarding the billing for the events I attended last month. There seems to be a discrepancy in the charges, and I would like to understand the breakdown of the costs. Could you please provide a detailed billing statement? Thank you.\"\n",
    "\n",
    "class VectorDocument(BaseModel):\n",
    "    \"\"\"The contents of a retrieved LangChain Document.\"\"\"\n",
    "    id: int\n",
    "    page_content: str\n",
    "    tags: str\n",
    "\n",
    "class SearchResultItem(BaseModel):\n",
    "    \"\"\"Represents the tuple (Document, score) as a JSON object.\"\"\"\n",
    "    document: VectorDocument = Field(description=\"The document content and metadata.\")\n",
    "    score: float = Field(description=\"The L2 distance.\")\n",
    "\n",
    "class QueryAttributeOutput(BaseModel):\n",
    "    retrival: Annotated[List[SearchResultItem], Field(description=\"List of retrieved document and score pairs.\")]\n",
    "\n",
    "results = vector_store.similarity_search_with_score(\n",
    "    query=query,\n",
    "    k=num_retrieval,\n",
    ")\n",
    "\n",
    "# Convert the output to recognizable types for Pydantic to work\n",
    "retrival = []\n",
    "for doc, score in results:\n",
    "    retrival.append({\n",
    "        \"document\": {\n",
    "            \"id\": int(doc.id),\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"tags\": doc.metadata.get(\"tags\")\n",
    "        },\n",
    "        \"score\": score\n",
    "    })\n",
    "\n",
    "QueryAttributeOutput(retrival=retrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0b0b9295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bdabab55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryAttributeOutput(retrival=[SearchResultItem(document=VectorDocument(id=19, page_content='How to Handle Billing Discrepancies: For billing issues:\\n\\n- Review your billing history in the CultPass app\\n- Contact support if discrepancies are found\\n- Provide transaction details for faster resolution\\n\\n**Suggested phrasing:**\\n\"If you notice any billing discrepancies, review your history and contact support with transaction details.\"', metadata={'tags': 'billing, escalation, support'}), score=1.0406265258789062), SearchResultItem(document=VectorDocument(id=5, page_content='Understanding CultPass Pricing: CultPass offers flexible pricing options:\\n\\n- Monthly subscription: Access to 4 experiences per month\\n- Annual subscription: Discounted rate for a year-long access\\n- Additional fees may apply for premium events\\n\\n**Suggested phrasing:**\\n\"Explore our flexible pricing plans, including monthly and annual subscriptions. Note that some premium events may incur additional fees.\"', metadata={'tags': 'pricing, subscription, benefits'}), score=1.2658634185791016), SearchResultItem(document=VectorDocument(id=14, page_content='Handling Event Overbooking: In case of overbooking:\\n\\n- Notify affected users via email\\n- Offer alternative dates or events\\n- Provide a refund if no alternatives are suitable\\n\\n**Suggested phrasing:**\\n\"We apologize for any inconvenience due to overbooking. Please check your email for alternative options or refund details.\"', metadata={'tags': 'events, booking, cancelation'}), score=1.321397066116333), SearchResultItem(document=VectorDocument(id=11, page_content='How to Access Premium Events: Accessing premium events requires:\\n\\n- A valid CultPass subscription\\n- Payment of any additional fees\\n- Booking through the CultPass app\\n\\n**Suggested phrasing:**\\n\"Access premium events by ensuring your subscription is active and any additional fees are paid. Book through the app.\"', metadata={'tags': 'access, events, booking'}), score=1.3554763793945312), SearchResultItem(document=VectorDocument(id=1, page_content='What\\'s Included in a CultPass Subscription: Each user is entitled to 4 cultural experiences per month, which may include:\\n- Art exhibitions\\n- Museum entries\\n- Music concerts\\n- Film screenings and more\\n\\nSome premium experiences may require an additional fee (visible in the app).\\n\\n**Suggested phrasing:**\\n\"Your CultPass subscription includes 4 curated experiences each month. You can choose from museums, concerts, film events and more. Premium events may have an extra cost, which is shown during reservation.\"', metadata={'tags': 'subscription, benefits, pricing, access'}), score=1.406754970550537)])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QueryAttributeOutput(retrival=retrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a5f4af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Hi, I have a question regarding the billing for the events I attended last month. There seems to be a discrepancy in the charges, and I would like to understand the breakdown of the costs. Could you please provide a detailed billing statement? Thank you.\"\n",
    "prompt_template = \"\"\"\n",
    "    You are an expert evaluating the intent of IT ticket submitted by users and you are suppose to\n",
    "    classify it as one of the following intent: ['how_to', 'troubleshooting_basic', 'status_check',\n",
    "    'security', 'refund_high', 'outage', 'others'].\n",
    "\n",
    "    Output a single one of the above intent in the list, do not provide any explanation or description\n",
    "    to your answer.\n",
    "\n",
    "    Here is the user query:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables = [\"query\"],\n",
    "    ).invoke({\"query\": query})\n",
    "response = llm_medium.with_structured_output(IntentOutput).invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5eba1a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Hi, I have a question regarding the billing for the events I attended last month. There seems to be a discrepancy in the charges, and I would like to understand the breakdown of the costs. Could you please provide a detailed billing statement? Thank you.\"\n",
    "query = \"You are so useless!\"\n",
    "prompt_template = \"\"\"\n",
    "    You are an expert in marketing and assessing the sentiment of the customer.  You need to provide a score between\n",
    "    -1 to 1 where -1 is upset/angry/negative where 1 is happy/optimisitic/positive.\n",
    "\n",
    "    Output a single number, do not provide any explanation or description to your answer.\n",
    "\n",
    "    Here is the user query:\n",
    "    {query}\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables = [\"query\"],\n",
    "    ).invoke({\"query\": query})\n",
    "response = llm_medium.with_structured_output(SentimentOutput).invoke(prompt)\n",
    "sentiment = response.sentiment\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a4410d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def knowledge_retrival(query: str):\n",
    "    \"\"\"\n",
    "    Evaluate whether the knowledge base has enough content to address the query and return the relevant content indices.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user query string\n",
    "    \n",
    "    Returns:\n",
    "        retrival (List[tuple[Document,float]]): Response from the ChromaDB database for the retrieved documents and the corresponding\n",
    "                                                distance measure.\n",
    "    \"\"\"\n",
    "    results = vector_store.similarity_search_with_score(\n",
    "        query=query,\n",
    "        k=num_retrieval,\n",
    "    )\n",
    "\n",
    "    # Convert the output to recognizable types for Pydantic to work\n",
    "    retrival = []\n",
    "    for doc, score in results:\n",
    "        safe_metadata = json.loads(json.dumps(doc.metadata, default=str))\n",
    "        retrival.append({\n",
    "            \"document\": {\n",
    "                \"id\": int(doc.id),\n",
    "                \"page_content\": doc.page_content,\n",
    "                \"tags\": doc.metadata.get(\"tags\",\"\")\n",
    "            },\n",
    "            \"score\": score\n",
    "        })\n",
    "    return retrival\n",
    "\n",
    "class VectorDocument(BaseModel):\n",
    "    \"\"\"The contents of a retrieved LangChain Document.\"\"\"\n",
    "    id: int\n",
    "    page_content: str\n",
    "    tags: str\n",
    "\n",
    "class SearchResultItem(BaseModel):\n",
    "    \"\"\"Represents the tuple (Document, score) as a JSON object.\"\"\"\n",
    "    document: VectorDocument = Field(description=\"The document content and metadata.\")\n",
    "    score: float = Field(description=\"The L2 distance.\")\n",
    "\n",
    "class QueryAttributeOutput(BaseModel):\n",
    "    retrival: List[SearchResultItem] = Field(min_length=num_retrieval, description=\"List of retrieved document and score pairs.\")\n",
    "\n",
    "query = \"Hi, I have a question regarding the billing for the events I attended last month. There seems to be a discrepancy in the charges, and I would like to understand the breakdown of the costs. Could you please provide a detailed billing statement? Thank you.\"\n",
    "llm_initial_evaluator = llm_medium.bind_tools([knowledge_retrival])\n",
    "# Evaluate Retrival Score, Intent, Sentiment of the Query\n",
    "prompt_template = \"\"\"\n",
    "    You are a IT ticket evaluation agent, given the user query:\n",
    "    {query}\n",
    "\n",
    "    Use `knowledge_retrival` to extract \"retrival\" attribute from the ChromaDB knowledge database\n",
    "    \"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template = prompt_template,\n",
    "    input_variables = [\"query\"],\n",
    "    ).invoke({\"query\": query})\n",
    "\n",
    "response = llm_initial_evaluator.with_structured_output(QueryAttributeOutput).invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "eeebf3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response.retrival)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5493d159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SearchResultItem(document=VectorDocument(id=1, page_content='Billing Statement for Events Attended in September 2023:\\n\\n1. Event Name: Tech Conference 2023\\n   Date: September 10, 2023\\n   Charges: $150.00\\n   Description: Full access to all sessions and networking events.\\n\\n2. Event Name: Workshop on AI\\n   Date: September 15, 2023\\n   Charges: $75.00\\n   Description: Hands-on workshop with expert speakers.\\n\\n3. Event Name: Networking Dinner\\n   Date: September 20, 2023\\n   Charges: $50.00\\n   Description: Dinner with keynote speakers and industry leaders.\\n\\nTotal Charges: $275.00\\n\\nIf you have any further questions or need additional details, please let us know!', tags='billing, events, September 2023'), score=0.95),\n",
       " SearchResultItem(document=VectorDocument(id=2, page_content='To address billing discrepancies, please review the following:\\n\\n- Ensure that you have registered for the events you attended.\\n- Check for any promotional codes or discounts that may apply.\\n- Verify the payment method used and any associated fees.\\n\\nIf you still find discrepancies, please contact our billing department directly for assistance.', tags='billing, discrepancies, support'), score=0.85),\n",
       " SearchResultItem(document=VectorDocument(id=3, page_content='For a detailed breakdown of your billing statement, please provide the following information:\\n\\n- Your full name\\n- Email address used for registration\\n- List of events you attended\\n\\nOnce we have this information, we can generate a detailed statement for you.', tags='billing, request information'), score=0.8),\n",
       " SearchResultItem(document=VectorDocument(id=4, page_content='If you believe there is an error in your billing, please follow these steps:\\n\\n1. Review your event registration confirmations.\\n2. Compare the charges listed in your account with the events attended.\\n3. Contact customer support with your findings for further investigation.', tags='billing, error, support'), score=0.75),\n",
       " SearchResultItem(document=VectorDocument(id=5, page_content='For any billing inquiries, you can reach out to our support team via email at support@example.com or call us at (123) 456-7890. We are here to help you resolve any issues you may have regarding your billing statement.', tags='billing, support, contact'), score=0.7)]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4d4454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.11_udacity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
